\providecommand\numberofexercises{}
\XSIM{solution-body}{exercise-1=={\par \begin {equation} \lambda _{\max } = \dots \end {equation} \par }||exercise-2=={\par \begin {equation} \lambda _{\max } = \dots \end {equation} \par }||exercise-3=={\par }||exercise-4=={\par }||exercise-5=={\par }||exercise-6=={\par }||exercise-7=={\par }||exercise-8=={\par }||exercise-9=={\par \begin {figure} \centering \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{example-image-golden}} \centerline {Periodogram ($N=200$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{example-image-golden}} \centerline {Periodogram ($N=500$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{example-image-golden}} \centerline {Periodogram ($N=1000$)} \end {minipage} \vskip 1em \caption {Barlett's periodograms of a Gaussian white noise (see Question~\ref {q:barlett}).} \label {fig:barlett} \end {figure} \par }||exercise-10=={\par }||exercise-11=={\begin {figure} \centering \begin {minipage}[t]{\textwidth } \centerline {\includegraphics [width=0.6\textwidth ]{example-image-golden}} \centerline {Badly classified healthy step} \end {minipage} \vskip 1em \begin {minipage}[t]{\textwidth } \centerline {\includegraphics [width=0.6\textwidth ]{example-image-golden}} \centerline {Badly classified non-healthy step} \end {minipage} \vskip 1em \caption {Examples of badly classified steps (see Question~\ref {q:class-errors}).} \label {fig:class-errors} \end {figure}}}
\XSIM{exercise-body}{exercise-1=={Consider the following Lasso regression: \begin {equation}\label {eq:lasso} \min _{\beta \in \RR ^p} \frac {1}{2}\norm {y-X\beta }^2_2 \quad + \quad \lambda \norm {\beta }_1 \end {equation} where $y\in \RR ^n$ is the response vector, $X\in \RR ^{n\times p}$ the design matrix, $\beta \in \RR ^p$ the vector of regressors and $\lambda >0$ the smoothing parameter. \par Show that there exists $\lambda _{\max }$ such that the minimizer of~\eqref {eq:lasso} is $\mathbf {0}_p$ (a $p$-dimensional vector of zeros) for any $\lambda > \lambda _{\max }$.}||exercise-2=={For a univariate signal $\mathbf {x}\in \mathbb {R}^n$ with $n$ samples, the convolutional dictionary learning task amounts to solving the following optimization problem: \par \begin {equation} \min _{(\mathbf {d}_k)_k, (\mathbf {z}_k)_k \\ \norm {\mathbf {d}_k}_2^2\leq 1} \quad \norm {\mathbf {x} - \sum _{k=1}^K \mathbf {z}_k * \mathbf {d}_k }^2_2 \quad + \quad \lambda \sum _{k=1}^K \norm {\mathbf {z}_k}_1 \end {equation} \par where $\mathbf {d}_k\in \mathbb {R}^L$ are the $K$ dictionary atoms (patterns), $\mathbf {z}_k\in \mathbb {R}^{N-L+1}$ are activations signals, and $\lambda >0$ is the smoothing parameter. \par Show that \begin {itemize} \item for a fixed dictionary, the sparse coding problem is a lasso regression (explicit the response vector and the design matrix); \item for a fixed dictionary, there exists $\lambda _{\max }$ (which depends on the dictionary) such that the sparse codes are only 0 for any $\lambda > \lambda _{\max }$. \end {itemize}}||exercise-3=={In this question, let $X_n$ ($n=0,\dots ,N-1)$ be a Gaussian white noise. \par \begin {itemize} \item Calculate the associated autocovariance function and power spectrum. (By analogy with the light, this process is called ``white'' because of the particular form of its power spectrum.) \end {itemize} \par }||exercise-4=={A natural estimator for the autocorrelation function is the sample autocovariance \begin {equation} \hat {\gamma }(\tau ) := (1/N) \sum _{n=0}^{N-\tau -1} X_n X_{n+\tau } \end {equation} for $\tau =0,1,\dots ,N-1$ and $\hat {\gamma }(\tau ):=\hat {\gamma }(-\tau )$ for $\tau =-(N-1),\dots ,-1$. \begin {itemize} \item Show that $\hat {\gamma }(\tau )$ is a biased estimator of $\gamma (\tau )$ but asymptotically unbiased. What would be a simple way to de-bias this estimator? \end {itemize} \par }||exercise-5=={Define the discrete Fourier transform of the random process $\{X_n\}_n$ by \begin {equation} J(f) := (1/\sqrt {N})\sum _{n=0}^{N-1} X_n e^{-2\pi \iu f n/f_s} \end {equation} The \textit {periodogram} is the collection of values $|J(f_0)|^2$, $|J(f_1)|^2$, \dots , $|J(f_{N/2})|^2$ where $f_k = f_s k/N$. (They can be efficiently computed using the Fast Fourier Transform.) \begin {itemize} \item Write $|J(f_k)|^2$ as a function of the sample autocovariances. \item For a frequency $f$, define $f^{(N)}$ the closest Fourier frequency $f_k$ to $f$. Show that $|J(f^{(N)})|^2$ is an asymptotically unbiased estimator of $S(f)$ for $f>0$. \end {itemize}}||exercise-6=={\label {ex:wn-exp} In this question, let $X_n$ ($n=0,\dots ,N-1)$ be a Gaussian white noise with variance $\sigma ^2=1$ and set the sampling frequency to $f_s=1$ Hz \begin {itemize} \item For $N\in \{200, 500, 1000\}$, compute the \textit {sample autocovariances} ($\hat {\gamma }(\tau )$ vs $\tau $) for 100 simulations of $X$. Plot the average value as well as the average $\pm $, the standard deviation. What do you observe? \item For $N\in \{200, 500, 1000\}$, compute the \textit {periodogram} ($|J(f_k)|^2$ vs $f_k$) for 100 simulations of $X$. Plot the average value as well as the average $\pm $, the standard deviation. What do you observe? \end {itemize} Add your plots to Figure~\ref {fig:wn-exp}. \par \begin {figure} \centering \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{example-image-golden}} \centerline {Autocovariance ($N=200$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{example-image-golden}} \centerline {Autocovariance ($N=500$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{example-image-golden}} \centerline {Autocovariance ($N=1000$)} \end {minipage} \vskip 1em \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{example-image-golden}} \centerline {Periodogram ($N=200$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{example-image-golden}} \centerline {Periodogram ($N=500$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{example-image-golden}} \centerline {Periodogram ($N=1000$)} \end {minipage} \vskip 1em \caption {Autocovariances and periodograms of a Gaussian white noise (see Question~\ref {ex:wn-exp}).} \label {fig:wn-exp} \end {figure} \par }||exercise-7=={We want to show that the estimator $\hat {\gamma }(\tau )$ is consistent, \ie it converges in probability when the number $N$ of samples grows to $\infty $ to the true value ${\gamma }(\tau )$. In this question, assume that $X$ is a wide-sense stationary \textit {Gaussian} process. \begin {itemize} \item Show that for $\tau >0$ \begin {equation} \text {var}(\hat {\gamma }(\tau )) = (1/N) \sum _{n=-(N-\tau -1)}^{n=N-\tau -1} \left (1 - \frac {\tau + |n|}{N}\right ) \left [\gamma ^2(n) + \gamma (n-\tau )\gamma (n+\tau )\right ]. \end {equation} (Hint: if $\{Y_1, Y_2, Y_3, Y_4\}$ are four centered jointly Gaussian variables, then $\mathbb {E}[Y_1 Y_2 Y_3 Y_4] = \mathbb {E}[Y_1 Y_2]\mathbb {E}[Y_3 Y_4] + \mathbb {E}[Y_1 Y_3]\mathbb {E}[Y_2 Y_4] + \mathbb {E}[Y_1 Y_4]\mathbb {E}[Y_2 Y_3]$.) \item Conclude that $\hat {\gamma }(\tau )$ is consistent. \end {itemize}}||exercise-8=={Assume that $X$ is a Gaussian white noise (variance $\sigma ^2$) and let $A(f):=\sum _{n=0}^{N-1} X_n \cos (-2\pi f n/f_s$ and $B(f):=\sum _{n=0}^{N-1} X_n \sin (-2\pi f n/f_s$. Observe that $J(f) = (1/N) (A(f) + \iu B(f))$. \begin {itemize} \item Derive the mean and variance of $A(f)$ and $B(f)$ for $f=f_0, f_1,\dots , f_{N/2}$ where $f_k=f_s k/N$. \item What is the distribution of the periodogram values $|J(f_0)|^2$, $|J(f_1)|^2$, \dots , $|J(f_{N/2})|^2$. \item What is the variance of the $|J(f_k)|^2$? Conclude that the periodogram is not consistent. \item Explain the erratic behavior of the periodogram in Question~\ref {ex:wn-exp} by looking at the covariance between the $|J(f_k)|^2$. \end {itemize} \par }||exercise-9=={\label {q:barlett} As seen in the previous question, the problem with the periodogram is the fact that its variance does not decrease with the sample size. A simple procedure to obtain a consistent estimate is to divide the signal into $K$ sections of equal durations, compute a periodogram on each section, and average them. Provided the sections are independent, this has the effect of dividing the variance by $K$. This procedure is known as Bartlett's procedure. \begin {itemize} \item Rerun the experiment of Question~\ref {ex:wn-exp}, but replace the periodogram by Barlett's estimate (set $K=5$). What do you observe? \end {itemize} Add your plots to Figure~\ref {fig:barlett}.}||exercise-10=={Combine the DTW and a k-neighbors classifier to classify each step. Find the optimal number of neighbors with 5-fold cross-validation and report the optimal number of neighbors and the associated F-score. Comment briefly.}||exercise-11=={\label {q:class-errors} Display on Figure~\ref {fig:class-errors} a badly classified step from each class (healthy/non-healthy).}}
\XSIM{goal}{exercise}{points}{0}
\XSIM{totalgoal}{points}{0}
\XSIM{goal}{exercise}{bonus-points}{0}
\XSIM{totalgoal}{bonus-points}{0}
\XSIM{order}{1||2||3||4||5||6||7||8||9||10||11}
\XSIM{use}{}
\XSIM{use!}{}
\XSIM{used}{exercise-1=={true}||exercise-2=={true}||exercise-3=={true}||exercise-4=={true}||exercise-5=={true}||exercise-6=={true}||exercise-7=={true}||exercise-8=={true}||exercise-9=={true}||exercise-10=={true}||exercise-11=={true}}
\XSIM{print}{}
\XSIM{print!}{}
\XSIM{printed}{exercise-1=={true}||exercise-2=={true}||exercise-3=={true}||exercise-4=={true}||exercise-5=={true}||exercise-6=={true}||exercise-7=={true}||exercise-8=={true}||exercise-9=={true}||exercise-10=={true}||exercise-11=={true}}
\XSIM{total-number}{11}
\XSIM{exercise}{11}
\XSIM{types}{exercise}
\XSIM{idtypes}{1=={exercise}||2=={exercise}||3=={exercise}||4=={exercise}||5=={exercise}||6=={exercise}||7=={exercise}||8=={exercise}||9=={exercise}||10=={exercise}||11=={exercise}}
\XSIM{collections}{exercise-1=={all exercises}||exercise-2=={all exercises}||exercise-3=={all exercises}||exercise-4=={all exercises}||exercise-5=={all exercises}||exercise-6=={all exercises}||exercise-7=={all exercises}||exercise-8=={all exercises}||exercise-9=={all exercises}||exercise-10=={all exercises}||exercise-11=={all exercises}}
\XSIM{collection:all exercises}{exercise-1||exercise-2||exercise-3||exercise-4||exercise-5||exercise-6||exercise-7||exercise-8||exercise-9||exercise-10||exercise-11}
\setcounter{totalexerciseinall exercises}{11}
\XSIM{id}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}}
\XSIM{ID}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}}
\XSIM{counter}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}}
\XSIM{counter-value}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}}
\XSIM{solution}{}
\XSIM{section-value}{exercise-1=={2}||exercise-2=={2}||exercise-3=={3}||exercise-4=={3}||exercise-5=={3}||exercise-6=={3}||exercise-7=={3}||exercise-8=={3}||exercise-9=={3}||exercise-10=={4}||exercise-11=={4}}
\XSIM{section}{exercise-1=={2}||exercise-2=={2}||exercise-3=={3}||exercise-4=={3}||exercise-5=={3}||exercise-6=={3}||exercise-7=={3}||exercise-8=={3}||exercise-9=={3}||exercise-10=={4}||exercise-11=={4}}
\XSIM{sectioning}{exercise-1=={{0}{2}{0}{0}{0}}||exercise-2=={{0}{2}{0}{0}{0}}||exercise-3=={{0}{3}{0}{0}{0}}||exercise-4=={{0}{3}{0}{0}{0}}||exercise-5=={{0}{3}{0}{0}{0}}||exercise-6=={{0}{3}{0}{0}{0}}||exercise-7=={{0}{3}{0}{0}{0}}||exercise-8=={{0}{3}{0}{0}{0}}||exercise-9=={{0}{3}{0}{0}{0}}||exercise-10=={{0}{4}{2}{0}{0}}||exercise-11=={{0}{4}{2}{0}{0}}}
\XSIM{subtitle}{}
\XSIM{points}{}
\XSIM{bonus-points}{}
\XSIM{page-value}{exercise-1=={1}||exercise-2=={2}||exercise-3=={2}||exercise-4=={3}||exercise-5=={3}||exercise-6=={3}||exercise-7=={4}||exercise-8=={4}||exercise-9=={5}||exercise-10=={6}||exercise-11=={7}}
\XSIM{page}{exercise-1=={1}||exercise-2=={2}||exercise-3=={2}||exercise-4=={3}||exercise-5=={3}||exercise-6=={3}||exercise-7=={4}||exercise-8=={4}||exercise-9=={5}||exercise-10=={6}||exercise-11=={7}}
\XSIM{tags}{}
\XSIM{topics}{}
\XSIM{userpoints}{}
\XSIM{bodypoints}{}
\XSIM{userbonus-points}{}
\XSIM{bodybonus-points}{}
